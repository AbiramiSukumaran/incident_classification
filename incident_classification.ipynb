pip install wordcloud

pip install scikit-learn-intelex

pip install modin[all]



pip install lxml

pip install pyodbc

pip install spacy

pip install bs4

pip install nltk

pip install tensorflow

pip install torch==1.5.0

pip install torchvision==0.6.0

pip install transformers==3.4.0

pip install bert-extractive-summarizer

!python -m spacy download en_core_web_sm

!pip install summarizer

This notebook has the list of steps for extracting and preprocessing your data set in preparation for Vertex AI models.

#@title Import dependencies

import re
import string
import os
import sys
import pyodbc
import pandas as pd
import numpy as np
import spacy
from spacy.lang.en.stop_words import STOP_WORDS as stopwords
from bs4 import BeautifulSoup
import unicodedata
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from tqdm import tqdm
from summarizer import Summarizer
model = Summarizer()

nlp = spacy.load('en_core_web_sm')

import nltk
from nltk.util import ngrams

nlp = spacy.load('en_core_web_sm')

stopwords -= {"do", "no", "not", "see", "cannot", "off", "show"}

import os
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"

pd.set_option('display.max_colwidth', None)
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

#@title Method to get word counts
def get_wordcounts(x):
    length = len(str(x).split())
    return length


def get_charcounts(x):
    s = x.split()
    x = ''.join(s)
    return len(x)

def get_avg_wordlength(x):
    if get_wordcounts(x) > 0:
        count = get_charcounts(x)/get_wordcounts(x)
    else:
        count = get_charcounts(x)
    return count

def get_stopwords_counts(x):
    l = len([t for t in x.split() if t in stopwords])
    return l

def get_hashtag_counts(x):
    l = len([t for t in x.split() if t.startswith('#')])
    return l

def get_mentions_counts(x):
    l = len([t for t in x.split() if t.startswith('@')])
    return l

def get_digit_counts(x):
    return len([t for t in x.split() if t.isdigit()])

def get_kb_number(x):
#     x = x.lower()
    return  re.findall(r'(kb[0-9]{6,})', x)

def get_uppercase_counts(x):
    return len([t for t in x.split() if t.isupper()])

#@title Method to remove HTML tags
def remove_html_tags(x):
    return BeautifulSoup(x, 'lxml').get_text().strip()

#@title Method to define and expand words
def cont_exp(x):
    contractions = { 
    "ain't": "am not",
    "aren't": "are not",
    "can't": "cannot",
    "can't've": "cannot have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would",
    "he'd've": "he would have",
    "he'll": "he will",
    "he'll've": "he will have",
    "he's": "he is",
    "how'd": "how did",
    "how'd'y": "how do you",
    "how'll": "how will",
    "how's": "how does",
    "i'd": "i would",
    "i'd've": "i would have",
    "i'll": "i will",
    "i'll've": "i will have",
    "i'm": "i am",
    "i've": "i have",
    "isn't": "is not",
    "it'd": "it would",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is",
    "let's": "let us",
    "ma'am": "madam",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "o'clock": "of the clock",
    "oughtn't": "ought not",
    "oughtn't've": "ought not have",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "shan't've": "shall not have",
    "she'd": "she would",
    "she'd've": "she would have",
    "she'll": "she will",
    "she'll've": "she will have",
    "she's": "she is",
    "should've": "should have",
    "shouldn't": "should not",
    "shouldn't've": "should not have",
    "so've": "so have",
    "so's": "so is",
    "that'd": "that would",
    "that'd've": "that would have",
    "that's": "that is",
    "there'd": "there would",
    "there'd've": "there would have",
    "there's": "there is",
    "they'd": "they would",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "to've": "to have",
    "wasn't": "was not"
    }

    if type(x) is str:
        for key in contractions:
            value = contractions[key]
            x = x.replace(key, value)
        return x
    else:
        return x

#@title Method to extract emails
def get_emails(x):
    emails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\.[a-z0-9+_-]+\b)', x)
    counts = len(emails)

    return counts, emails

def remove_emails(x):
    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\.[a-z0-9+_-]+)',"", x)

def get_urls(x):
    urls = re.findall(r'(http|https|ftp|ssh)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', x)
    urls.extend(re.findall(r'([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', x))
    counts = len(urls)

#     return counts, 
    return urls

def remove_urls(x):
    x = re.sub(r'(http|https|ftp|ssh)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?', '' , x)
    return x

def remove_rt(x):
    return re.sub(r'\brt\b', '', x).strip()

#@title Method to remove nulls
def remove_na(x):
    x = re.sub(r'\bna\b', '', x).strip()
    x = re.sub(r'\bn\b', '', x).strip()
    return x

def remove_whitespaces(x):
    x = re.sub(' +', ' ', x)
    return x

def remove_common_text_postsummaryprocess(x):
    utext = remove_hi(x) 
    utext = remove_user(utext)
    return utext

#@title Method to remove digits
def remove_digits(text):
    pattern = r'[^a-zA-z.,!?/:;\"\'\s]' 
    return re.sub(pattern, '', text)
    
def remove_special_characters(text):
    # define the pattern to keep
    text = text.replace("_","")
    pat = r'[^a-zA-z0-9.,!?/:;\"\'\s]' 
    return re.sub(pat, '', text)

#@title Method to remove special characters
def remove_special_chars(x):
    x = re.sub(r'[^\w ]+', "", x)
    x = ' '.join(x.split())
    return x

#@title Method to remove accented characters
def remove_accented_chars(x):
    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return x

#@title Method to remove stop words
def remove_stopwords(x):
    return ' '.join([t for t in x.split() if t not in stopwords])

def make_base(x):
    x = str(x)
    x_list = []
    doc = nlp(x)

    for token in doc:
        if token.text not in ['feeds', 'leads', 'teams', 'tasks', 'events', 'dashboards', 'reports', 'cases', 'opportunities', 'ideas','courses','credentials', 'files', 'windows', 'notifications', 'managed', 'learning', 'logging','stuck', 'locked']:
            lemma = token.lemma_
#             print(lemma)
            if lemma == '-PRON-' or lemma == 'be':
                lemma = token.text
        else:
            lemma = token.text

        x_list.append(lemma)
    return ' '.join(x_list)

#@title Method to get unique statements
# def get_unique_sentences(number, x):
def get_unique_sentences(x):
#     print(number)
    x = str(x)
    x_list = []
    doc = nlp(x)
    
    #for doc in docs:
    for i, token in enumerate(doc.sents):
        if token.text.strip() not in x_list:
            x_list.append(token.text.strip())

    return ' '.join(x_list)

#@title Method to get sentence count
def get_sentence_count(x):
    x = str(x)
    doc = nlp(x)
    return len(list(doc.sents))

#@title Method to summarize text
def get_summary(x):
  sc = get_sentence_count(x)
  if sc <= 4:
      summary = x
  else:
      summary = model(x)
  return summary

print(get_summary("Hello this is a new beginning. There is. always. light."))

def get_value_counts(df, col):
    text = ' '.join(df[col])
    text = text.split()
    freq = pd.Series(text).value_counts()
    return freq

def remove_common_words(x, freq, n=20):
    fn = freq[:n]
    x = ' '.join([t for t in x.split() if t not in fn])
    return x

def remove_rarewords(x, freq, n=20):
    fn = freq.tail(n)
    x = ' '.join([t for t in x.split() if t not in fn])
    return x

#@title Method to check spelling
def spelling_correction(x):
    retvalue = []
    value = ''
    spell = SpellChecker()
    if x:
        x = ''.join(x)
        x = x.split(' ')
        misspelled = spell.unknown(x)
        for allword in x:
            if allword in misspelled:
                # Get the one `most likely` answer
                if allword not in (['okta','vpn', 'prehire', 'apac', 'ssrs', 'cva', 'cvd', 'ad', 'rdp','thycotic', 'vdi', 'wfm', 'runtime' ]):
                    value = spell.correction(allword)
                else:
                    value = allword
                # Get a list of `likely` options
            else:
                value = allword
            retvalue.append(value)         
        return ' '.join(retvalue)   
    else:
        return x

def replace_single_character(x):
    return re.sub(r"\b[a-zA-Z]\b", "", x)

def punct(text):
    token = RegexpTokenizer(r'\w+')
    # from token to alphabetic sequences
    text = token.tokenize(text)
    text = " ".join(text)
    return text

import pandas as pd

df = pd.read_csv('/content/sample_data/comcast_consumeraffairs_complaints.csv')
df = df[['text']]
df = df.dropna()

'''import modin.pandas as pd
from distributed import Client
client = Client()
os.environ["MODIN_ENGINE"] = "dask" '''


#%timeit df = pd.read_csv('/content/sample_data/comcast_consumeraffairs_complaints.csv')

df['cleaned_issue_description'] = df['text']

!pip install summarizers

from summarizers import Summarizers
summ = Summarizers()

print(summ("When I called the infinity customer service center to complain about the new slow performance speed the first question customer service asked was How fast is it now and why didn't you buy the previous offer for the faster service. I responded with What speed is my plan am I paying for now. I explained to customer service the actual test was less than half that. The customer service rep said she would resend a signal to my router. No improved speed occurred. I am getting screw from Comcast/Xfinity I turned on my hotspot on my cell phone to file this complaint. I am getting screw from Comcast/Xfinity. My internet speed is not what I pay for."))

#@title Invoke preprocessing methods
#Invoke preprocessing methods one by one

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: cont_exp(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: get_unique_sentences(x))

#df['sentence_count'] = df['cleaned_issue_description'].apply(lambda x: get_sentence_count(x))

df['summary'] = df['text']

df['summary'] = df['summary'].apply(lambda x: remove_special_characters(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: remove_digits(x))
df['cleaned_issue_description_postprocess'] = df['cleaned_issue_description'].apply(lambda x: remove_digits(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: punct(x))
df['cleaned_issue_description_postprocess'] = df['cleaned_issue_description_postprocess'].apply(lambda x: punct(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: remove_accented_chars(x))
df['cleaned_issue_description_postprocess'] = df['cleaned_issue_description_postprocess'].apply(lambda x: remove_accented_chars(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: replace_single_character(x))
df['cleaned_issue_description_postprocess'] = df['cleaned_issue_description_postprocess'].apply(lambda x: replace_single_character(x))

df['cleaned_issue_description'] = df['cleaned_issue_description'].apply(lambda x: remove_special_characters(x))
df['cleaned_issue_description_postprocess'] = df['cleaned_issue_description_postprocess'].apply(lambda x: remove_special_characters(x))

df['word_count'] = df['cleaned_issue_description'].apply(lambda x: get_wordcounts(x))

df['char_count'] = df['cleaned_issue_description'].apply(lambda x: get_charcounts(x))

df['average_word_length'] = df['cleaned_issue_description'].apply(lambda x: get_avg_wordlength(x))

#@title Join all input texts to form one big input string
text = ' '.join(df['cleaned_issue_description'])

import nltk
nltk.download('punkt')> nltk.download('punkt')

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

from wordcloud import WordCloud
import matplotlib.pyplot as plt
%matplotlib inline

#@title Generate Word Cloud to analyze the context and key intents
if text:
    wc = WordCloud(width=1200, height=800, max_font_size=100, background_color="white").generate(text)
    plt.figure(figsize=(20,10))
    plt.imshow(wc)
    plt.axis('off')
    plt.show()
    

from pprint import pprint

#@title List Word Cloud words with their weight in descending order and pick top 5-10 for labeling
key_value = WordCloud().process_text(text)
pprint(sorted(key_value.items(), key =
             lambda kv:(kv[1], kv[0])))  

def generateWC(text):
  keys = []
  key_value = WordCloud().process_text(text)
  for key in key_value:
    keys.append(key)
  return keys


checkdf = df.copy()

#@title Top Intents identified
issuelist = ['Speed Issue', 'Interruption', 'Poor Service', 'Overcharge']


#@title Create label dictionary
issuedict = {
    'Speed Issue' : [
    'slow', 'speed', 'running', 'internet speed', 'speed internet', 'high speed' 
    ],
    'Interruption' : [
    'broken', 'reset', 'failed', 'network', 'trouble', 'connect', 'internet connection', 'wifi','internet cable', 'router', 'disconnect', 'outage', 'signal', 'disconnected'
    ],
    'Poor Service' : [
    'multiple time', 'poor service', 'scheduled appointment', 'suck', 'customer support', 'stupid', 'showed called', 'treated', 'poor customer', 'weeks later', 'missed', 'service time', 'frustrating', 'Comcast worst', 
    'worst company', 'incompetent', 'Comcast technician', 'early termination',  'frustration', 'termination fee', 'worse', 'service center', 'horrible', 'service tech', 'frustrated', 'resolve issue', 'Comcast rep', 
    'canceled', 'basic cable', 'switched', 'upset', 'service representative', 'tech support', 'ticket', 'support', 'repair', 'installer', 'TV service', 'Verizon', 'ridiculous', 'hours phone', 'refused', 'promise',  
    'cancel service', 'cancelled', 'speak supervisor', 'employee', 'mistake', 'rude', 'lie', 'complaint', 'customer service'
    ],
    'Overcharge' : [
    'overcharged',  'overcharge', 'afford', 'pricing', 'told pay', 'paying month', 'fraud', 'frustrating', 'installation fee', 'late fee', 'collection agency', 'credited', 'monthly bill', 'owed', 'paid bill', 
    'claim', 'got bill', 'balance', 'agreement', 'contract', 'bill month', 'billed', 'dollar', 'received bill', 'charging', 'refund', 'rate', 'pay bill', 'billing', 'paying', 'charged', 'price', 'fee', 'paid', 
    'payment', 'pay', 'charge'
    ]
}

#@title Helper Method: If text matches a dictionary value, return its key
def textcheck(text):
  for key, val in issuedict.items():
    if text in val:
      return key
  return ''

#@title Generate Word Cloud for each input text in your sample
checkdf['Workd_Cloud_Each'] = checkdf['cleaned_issue_description_postprocess'].apply(lambda x: generateWC(x))

print(checkdf['Workd_Cloud_Each'])

checkdf['label'] = checkdf['cleaned_issue_description_postprocess'].apply(lambda x: generateWC(x))
print(checkdf['label'])

#@title If individual texts Word Cloud words are present in the labeled list, assign that label
label = []

for i in checkdf.index:
  text = checkdf['Workd_Cloud_Each'][i]
  for texts in text:
    temp = textcheck(texts)
    if temp != '':
      label.append(temp)
      checkdf['label'][i] = temp
      break
    else:
      checkdf['label'][i] = np.nan

checkdf = checkdf.dropna()
print(len(checkdf))

checkdf = checkdf.dropna(subset=['label'])
print(checkdf['label'].isnull().sum())
print(len(checkdf))

# iterating the columns
for col in checkdf.columns:
    print(col)

JSONtext = ''.join(checkdf['cleaned_issue_description_postprocess'])

#@title Method to convert list into dictionary for JSONL format result
def convert(lst):
    result_dict = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}
    return result_dict

pip install xgboost

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.base import BaseEstimator

df = pd.DataFrame(checkdf, columns =['cleaned_issue_description_postprocess', 'label']) 
print(len(df))

df.head(10)

df.to_csv('incident_classification_postprocess.csv')

# from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Divide the data into 75% training and 25% testing data
train_data = df.cleaned_issue_description_postprocess[0:int(0.75*len(df))]
test_data = df.cleaned_issue_description_postprocess[int(0.75*len(df))+1:]
train_target = df.label[0:int(0.75*len(df))]
test_target = df.label[int(0.75*len(df))+1:]

# convert the text into numeric form, so that the ML model can be applied to them
stop_words = ['in', 'of', 'at', 'a', 'the']
ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)
ngram_vectorizer.fit(train_data)
X_train = ngram_vectorizer.transform(train_data)
X_test = ngram_vectorizer.transform(test_data)

# Train the model
model = LogisticRegression() # play around with the parameters in Logisticregression() to find the optimal parameters
model.fit(X_train, train_target)

# make predictions on the test data with the model, and check its accuracy
test_acc = accuracy_score(test_target, model.predict(X_test))
print('Test accuracy: {0:.2f}%'.format(100*test_acc))

from xgboost import XGBClassifier
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

model.fit(X_train, train_target)

XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',
              gamma=0, gpu_id=-1, importance_type='gain',
              interaction_constraints='', learning_rate=0.300000012,
              max_delta_step=0, max_depth=6, min_child_weight=1, missing=np.nan,
              monotone_constraints='()', n_estimators=100, n_jobs=16,
              num_parallel_tree=1, objective='multi:softprob', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,
              tree_method='exact', use_label_encoder=False,
              validate_parameters=1, verbosity=None)

y_pred = model.predict(X_test)

y_pred

accuracy = accuracy_score(test_target, y_pred)
accuracy
